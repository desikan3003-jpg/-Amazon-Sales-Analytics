{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa50d559",
   "metadata": {},
   "source": [
    "## Combined multiple data set into single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f14c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined CSV saved to: C:\\Users\\desik\\Desktop\\cleaned output\\amazon_india_combined_2015_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Folder containing your CSVs ---\n",
    "folder_path = r\"C:\\Users\\desik\\Desktop\\cleaned output\"\n",
    "\n",
    "# --- List of years to combine ---\n",
    "years = range(2015, 2026)\n",
    "\n",
    "# --- Build full file paths ---\n",
    "csv_files = [os.path.join(folder_path, f\"amazon_india_{year}_cleaned.csv\") for year in years]\n",
    "\n",
    "# --- Read and combine all CSVs ---\n",
    "combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# --- Save to a single CSV file ---\n",
    "output_path = os.path.join(folder_path, \"amazon_india_combined_2015_2025.csv\")\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Combined CSV saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ecce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined dataset loaded: (1127567, 38)\n",
      "‚úÖ MySQL table `amazon_transactions` created or verified.\n",
      "üéâ Data successfully loaded into `amazon_transactions` (1,127,567 rows).\n",
      "‚úÖ Index created on order_date\n",
      "‚úÖ Index created on customer_id\n",
      "‚úÖ Index created on product_id\n",
      "‚úÖ Index created on payment_method\n",
      "‚úÖ Index created on category\n",
      "‚úÖ MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import os\n",
    "\n",
    "# ‚úÖ Load Combined Dataset\n",
    "file_path = r\"C:\\Users\\desik\\Desktop\\cleaned output\\amazon_india_combined_2015_2025.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"‚úÖ Combined dataset loaded:\", df.shape)\n",
    "\n",
    "# ‚úÖ Clean column names for MySQL compatibility\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    "    .str.replace(\"(\", \"\")\n",
    "    .str.replace(\")\", \"\")\n",
    ")\n",
    "\n",
    "# ‚úÖ Connect to MySQL\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Revdesi@2302\",\n",
    "    database=\"amazon_db\",\n",
    "    allow_local_infile=True\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SET GLOBAL local_infile = 1;\")\n",
    "\n",
    "# ‚úÖ Create Table Automatically\n",
    "table_name = \"amazon_transactions\"\n",
    "\n",
    "# Generate CREATE TABLE dynamically\n",
    "columns_with_types = []\n",
    "for col in df.columns:\n",
    "    # Simple inference\n",
    "    dtype = df[col].dtype\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        mysql_type = \"INT\"\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        mysql_type = \"FLOAT\"\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        mysql_type = \"BOOLEAN\"\n",
    "    else:\n",
    "        mysql_type = \"VARCHAR(255)\"\n",
    "    columns_with_types.append(f\"`{col}` {mysql_type}\")\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    {', '.join(columns_with_types)}\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_query)\n",
    "print(f\"‚úÖ MySQL table `{table_name}` created or verified.\")\n",
    "\n",
    "# ‚úÖ Save temp CSV for fast MySQL import\n",
    "temp_csv_path = os.path.join(os.getcwd(), \"temp_combined.csv\")\n",
    "df.to_csv(temp_csv_path, index=False)\n",
    "\n",
    "# ‚úÖ Load data using LOCAL INFILE\n",
    "load_query = f\"\"\"\n",
    "LOAD DATA LOCAL INFILE '{temp_csv_path.replace(\"\\\\\", \"/\")}'\n",
    "INTO TABLE {table_name}\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n'\n",
    "IGNORE 1 ROWS;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(load_query)\n",
    "conn.commit()\n",
    "print(f\"üéâ Data successfully loaded into `{table_name}` ({len(df):,} rows).\")\n",
    "\n",
    "# ‚úÖ Create essential indexes for faster queries\n",
    "indexes = [\n",
    "    (\"idx_order_date\", \"order_date\"),\n",
    "    (\"idx_customer_id\", \"customer_id\"),\n",
    "    (\"idx_product_id\", \"product_id\"),\n",
    "    (\"idx_payment_method\", \"payment_method\"),\n",
    "    (\"idx_category\", \"category\")\n",
    "]\n",
    "\n",
    "for idx_name, col_name in indexes:\n",
    "    try:\n",
    "        cursor.execute(f\"CREATE INDEX {idx_name} ON {table_name}({col_name});\")\n",
    "        print(f\"‚úÖ Index created on {col_name}\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Index already exists for {col_name}\")\n",
    "\n",
    "# ‚úÖ Cleanup\n",
    "os.remove(temp_csv_path)\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"‚úÖ MySQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad189868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to MySQL...\n",
      "üì• Reading CSV data...\n",
      "‚úÖ Loaded 1,127,567 valid rows from CSV.\n",
      "üßπ Dropping existing tables if any...\n",
      "üß± Creating tables...\n",
      "‚úÖ Tables created successfully.\n",
      "üß© Preparing dimension tables...\n",
      "üöÄ Loading data into MySQL...\n",
      "‚úÖ Loaded customers: 459,885 rows\n",
      "‚úÖ Loaded products: 2,579 rows\n",
      "‚úÖ Loaded time_dimension: 4,828 rows\n",
      "‚úÖ Loaded transactions: 1,127,567 rows\n",
      "‚öôÔ∏è Creating indexes...\n",
      "‚úÖ Created index: idx_order_date\n",
      "‚úÖ Created index: idx_customer_id\n",
      "‚úÖ Created index: idx_product_id\n",
      "‚úÖ Created index: idx_category_subcategory\n",
      "‚úÖ Created index: idx_customer_state\n",
      "‚úÖ Created index: idx_year_month\n",
      "üéâ Data successfully loaded into amazonsales_db with optimized schema and indexes.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Revdesi@2302\",\n",
    "    \"database\": \"amazonsales_db\",\n",
    "    \"allow_local_infile\": True,\n",
    "    \"ssl_disabled\": True  # ‚úÖ Disable SSL for local connections\n",
    "}\n",
    "\n",
    "CSV_PATH = r\"C:\\Users\\desik\\Desktop\\cleaned output\\amazon_india_combined_2015_2025.csv\"\n",
    "\n",
    "# --- CONNECT TO MYSQL ---\n",
    "print(\"üîó Connecting to MySQL...\")\n",
    "conn = mysql.connector.connect(**DB_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Enable local file import and adjust server settings\n",
    "cursor.execute(\"SET GLOBAL local_infile = 1;\")\n",
    "cursor.execute(\"SET SESSION sql_mode = '';\")\n",
    "cursor.execute(\"SET GLOBAL max_allowed_packet = 512*1024*1024;\")\n",
    "cursor.execute(\"SET GLOBAL net_buffer_length = 1048576;\")\n",
    "\n",
    "# --- READ CSV ---\n",
    "print(\"üì• Reading CSV data...\")\n",
    "df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"order_date\"])\n",
    "print(f\"‚úÖ Loaded {len(df):,} valid rows from CSV.\")\n",
    "\n",
    "# --- DROP OLD TABLES ---\n",
    "print(\"üßπ Dropping existing tables if any...\")\n",
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 0;\")\n",
    "for tbl in [\"transactions\", \"customers\", \"products\", \"time_dimension\"]:\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {tbl};\")\n",
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 1;\")\n",
    "conn.commit()\n",
    "\n",
    "# --- CREATE TABLES ---\n",
    "print(\"üß± Creating tables...\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE customers (\n",
    "        customer_id VARCHAR(100) PRIMARY KEY,\n",
    "        customer_city VARCHAR(100),\n",
    "        customer_state VARCHAR(100),\n",
    "        customer_tier VARCHAR(50),\n",
    "        customer_spending_tier VARCHAR(50),\n",
    "        customer_age_group VARCHAR(50)\n",
    "    ) ENGINE=InnoDB;\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE products (\n",
    "        product_id VARCHAR(100) PRIMARY KEY,\n",
    "        product_name TEXT,\n",
    "        category VARCHAR(255),\n",
    "        subcategory VARCHAR(255),\n",
    "        brand VARCHAR(255),\n",
    "        product_weight_kg DECIMAL(10,3),\n",
    "        is_prime_eligible TINYINT(1),\n",
    "        product_rating DECIMAL(3,2)\n",
    "    ) ENGINE=InnoDB;\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE time_dimension (\n",
    "        date DATE PRIMARY KEY,\n",
    "        year INT,\n",
    "        month INT,\n",
    "        day INT,\n",
    "        quarter INT,\n",
    "        week INT,\n",
    "        day_of_week VARCHAR(20),\n",
    "        is_weekend TINYINT(1),\n",
    "        is_holiday TINYINT(1),\n",
    "        holiday_name VARCHAR(255)\n",
    "    ) ENGINE=InnoDB;\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE transactions (\n",
    "        transaction_id VARCHAR(100) PRIMARY KEY,\n",
    "        order_date DATE,\n",
    "        customer_id VARCHAR(100),\n",
    "        product_id VARCHAR(100),\n",
    "        original_price_inr DECIMAL(10,2),\n",
    "        discount_percent DECIMAL(5,2),\n",
    "        discounted_price_inr DECIMAL(10,2),\n",
    "        quantity INT,\n",
    "        subtotal_inr DECIMAL(10,2),\n",
    "        delivery_charges DECIMAL(10,2),\n",
    "        final_amount_inr DECIMAL(10,2),\n",
    "        payment_method VARCHAR(100),\n",
    "        delivery_days INT,\n",
    "        delivery_type VARCHAR(100),\n",
    "        is_prime_member TINYINT(1),\n",
    "        is_festival_sale TINYINT(1),\n",
    "        festival_name VARCHAR(255),\n",
    "        customer_rating DECIMAL(3,2),\n",
    "        return_status VARCHAR(100),\n",
    "        order_month INT,\n",
    "        order_year INT,\n",
    "        order_quarter INT,\n",
    "        dup_key VARCHAR(255),\n",
    "        dup_count INT,\n",
    "        dup_status VARCHAR(100),\n",
    "        flag_for_review TINYINT(1),\n",
    "        FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n",
    "            ON DELETE SET NULL ON UPDATE CASCADE,\n",
    "        FOREIGN KEY (product_id) REFERENCES products(product_id)\n",
    "            ON DELETE SET NULL ON UPDATE CASCADE,\n",
    "        FOREIGN KEY (order_date) REFERENCES time_dimension(date)\n",
    "            ON DELETE SET NULL ON UPDATE CASCADE\n",
    "    ) ENGINE=InnoDB;\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Tables created successfully.\")\n",
    "\n",
    "# --- PREPARE DIMENSIONS ---\n",
    "print(\"üß© Preparing dimension tables...\")\n",
    "\n",
    "customers = df[[\n",
    "    \"customer_id\", \"customer_city\", \"customer_state\", \"customer_tier\",\n",
    "    \"customer_spending_tier\", \"customer_age_group\"\n",
    "]].drop_duplicates()\n",
    "\n",
    "products = df[[\n",
    "    \"product_id\", \"product_name\", \"category\", \"subcategory\", \"brand\",\n",
    "    \"product_weight_kg\", \"is_prime_eligible\", \"product_rating\"\n",
    "]].drop_duplicates()\n",
    "\n",
    "time_dimension = (\n",
    "    df[[\"order_date\", \"is_festival_sale\", \"festival_name\"]]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\"order_date\": \"date\"})\n",
    ")\n",
    "time_dimension[\"year\"] = time_dimension[\"date\"].dt.year\n",
    "time_dimension[\"month\"] = time_dimension[\"date\"].dt.month\n",
    "time_dimension[\"day\"] = time_dimension[\"date\"].dt.day\n",
    "time_dimension[\"quarter\"] = time_dimension[\"date\"].dt.quarter\n",
    "time_dimension[\"week\"] = time_dimension[\"date\"].dt.isocalendar().week\n",
    "time_dimension[\"day_of_week\"] = time_dimension[\"date\"].dt.day_name()\n",
    "time_dimension[\"is_weekend\"] = time_dimension[\"day_of_week\"].isin([\"Saturday\", \"Sunday\"]).astype(int)\n",
    "time_dimension[\"is_holiday\"] = time_dimension[\"is_festival_sale\"].fillna(False).astype(int)\n",
    "time_dimension[\"holiday_name\"] = time_dimension[\"festival_name\"].fillna(\"\")\n",
    "time_dimension = time_dimension.drop(columns=[\"is_festival_sale\", \"festival_name\"])\n",
    "\n",
    "transactions = df[[\n",
    "    \"transaction_id\", \"order_date\", \"customer_id\", \"product_id\",\n",
    "    \"original_price_inr\", \"discount_percent\", \"discounted_price_inr\",\n",
    "    \"quantity\", \"subtotal_inr\", \"delivery_charges\", \"final_amount_inr\",\n",
    "    \"payment_method\", \"delivery_days\", \"delivery_type\", \"is_prime_member\",\n",
    "    \"is_festival_sale\", \"festival_name\", \"customer_rating\", \"return_status\",\n",
    "    \"order_month\", \"order_year\", \"order_quarter\", \"dup_key\", \"dup_count\",\n",
    "    \"dup_status\", \"flag_for_review\"\n",
    "]].drop_duplicates()\n",
    "\n",
    "# --- LOAD FUNCTION (optimized) ---\n",
    "def load_table(df, table):\n",
    "    temp_path = os.path.join(os.getcwd(), f\"{table}_temp.csv\")\n",
    "    df.to_csv(temp_path, index=False, encoding=\"utf-8\", lineterminator=\"\\n\")  # ‚úÖ FIXED HERE\n",
    "    path = os.path.abspath(temp_path).replace(\"\\\\\", \"/\")\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        LOAD DATA LOCAL INFILE '{path}'\n",
    "        INTO TABLE {table}\n",
    "        FIELDS TERMINATED BY ',' \n",
    "        ENCLOSED BY '\"'\n",
    "        LINES TERMINATED BY '\\n'\n",
    "        IGNORE 1 ROWS;\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        print(f\"‚úÖ Loaded {table}: {len(df):,} rows\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"‚ùå Error loading {table}: {err}\")\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "# --- LOAD ALL TABLES ---\n",
    "print(\"üöÄ Loading data into MySQL...\")\n",
    "load_table(customers, \"customers\")\n",
    "load_table(products, \"products\")\n",
    "load_table(time_dimension, \"time_dimension\")\n",
    "load_table(transactions, \"transactions\")\n",
    "\n",
    "# --- INDEXES ---\n",
    "print(\"‚öôÔ∏è Creating indexes...\")\n",
    "index_list = [\n",
    "    (\"transactions\", \"idx_order_date\", \"CREATE INDEX idx_order_date ON transactions(order_date)\"),\n",
    "    (\"transactions\", \"idx_customer_id\", \"CREATE INDEX idx_customer_id ON transactions(customer_id)\"),\n",
    "    (\"transactions\", \"idx_product_id\", \"CREATE INDEX idx_product_id ON transactions(product_id)\"),\n",
    "    (\"products\", \"idx_category_subcategory\", \"CREATE INDEX idx_category_subcategory ON products(category, subcategory)\"),\n",
    "    (\"customers\", \"idx_customer_state\", \"CREATE INDEX idx_customer_state ON customers(customer_state)\"),\n",
    "    (\"time_dimension\", \"idx_year_month\", \"CREATE INDEX idx_year_month ON time_dimension(year, month)\")\n",
    "]\n",
    "\n",
    "for table, name, query in index_list:\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        print(f\"‚úÖ Created index: {name}\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"‚ö†Ô∏è {name} skipped ({err})\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"üéâ Data successfully loaded into amazonsales_db with optimized schema and indexes.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab4b72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting export process...\n",
      "Created output directory: data1/\n",
      "-> Exporting table: 'transactions'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desik\\AppData\\Local\\Temp\\ipykernel_34688\\744380054.py:44: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Successfully saved 1,127,567 rows to data1\\transactions.csv\n",
      "-> Exporting table: 'products'...\n",
      "   Successfully saved 2,004 rows to data1\\products.csv\n",
      "-> Exporting table: 'customers'...\n",
      "   Successfully saved 354,968 rows to data1\\customers.csv\n",
      "-> Exporting table: 'time_dimension'...\n",
      "   Successfully saved 4,015 rows to data1\\time_dimension.csv\n",
      "\n",
      "‚úÖ Export complete!\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import os\n",
    "\n",
    "# --- MySQL Connection Configuration ---\n",
    "# üö® IMPORTANT: Use the same credentials as your dashboard üö®\n",
    "MYSQL_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"root\", \n",
    "    \"password\": \"Revdesi@2302\", \n",
    "    \"database\": \"amazonsales_db\"\n",
    "}\n",
    "\n",
    "# --- Tables to Export ---\n",
    "TABLES = [\n",
    "    \"transactions\", \n",
    "    \"products\", \n",
    "    \"customers\", \n",
    "    \"time_dimension\"\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"data1\"\n",
    "\n",
    "def export_tables_to_csv():\n",
    "    \"\"\"Connects to MySQL, fetches data1 for specified tables, and saves them as CSV files.\"\"\"\n",
    "    \n",
    "    print(f\"Starting export process...\")\n",
    "    \n",
    "    # 1. Create output directory if it doesn't exist\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"Created output directory: {OUTPUT_DIR}/\")\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        # 2. Establish the connection\n",
    "        conn = mysql.connector.connect(**MYSQL_CONFIG)\n",
    "        \n",
    "        for table_name in TABLES:\n",
    "            print(f\"-> Exporting table: '{table_name}'...\")\n",
    "            query = f\"SELECT * FROM `{table_name}`;\"\n",
    "            \n",
    "            # 3. Read data into a pandas DataFrame\n",
    "            df = pd.read_sql(query, conn)\n",
    "            \n",
    "            # 4. Save the DataFrame to a CSV file\n",
    "            filepath = os.path.join(OUTPUT_DIR, f\"{table_name}.csv\")\n",
    "            df.to_csv(filepath, index=False)\n",
    "            \n",
    "            print(f\"   Successfully saved {len(df):,} rows to {filepath}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Export complete!\")\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"\\n‚ùå Database Error: {err}. Please check your MySQL configuration.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        if conn and conn.is_connected():\n",
    "            conn.close()\n",
    "            print(\"MySQL connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_tables_to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
